{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "def load_metadata():\n",
    "  file_path = Path(\"data/train.csv\")\n",
    "  return pd.read_csv(file_path)\n",
    "  \n",
    "metadata = load_metadata()\n",
    "\n",
    "def extract_eeg():\n",
    "  eeg_dir = Path(\"../data/eeg\")\n",
    "  tarball_path = Path(\"data/eeg.tar.gz\")\n",
    "  if not tarball_path.is_file():\n",
    "    url = 'https://dl.dropboxusercontent.com/scl/fi/5sina48c4naaxv6uze0fv/eeg.tar.gz?rlkey=r7ec191extynfcm8fy0tsiws5&dl=0'\n",
    "    urllib.request.urlretrieve(url, tarball_path)\n",
    "    with tarfile.open(tarball_path) as eeg_tarball:\n",
    "      eeg_tarball.extractall()\n",
    "    \n",
    "extract_eeg()\n",
    "\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from src.utils import compute_signal_hash\n",
    "\n",
    "channel_order = ['Fp1', 'Fp2',\n",
    "            'F7', 'F3', 'Fz', 'F4', 'F8', \n",
    "            'T3', 'C3', 'Cz', 'C4', 'T4', \n",
    "            'T5', 'P3', 'Pz', 'P4', 'T6', \n",
    "            'O1', 'O2',\n",
    "          ]\n",
    "\n",
    "def load_signals(metadata):\n",
    "  rows = len(metadata)\n",
    "  eeg_list = []\n",
    "\n",
    "  for row in range(0,rows):\n",
    "    sample = metadata.iloc[row]\n",
    "    f_name = f'data/eeg/{sample.eeg_id}.parquet'\n",
    "    eeg = pd.read_parquet(f_name)[channel_order]\n",
    "    eeg_offset = int(sample.eeg_label_offset_seconds)\n",
    "\n",
    "    eeg['eeg_id'] = str(sample.eeg_id)\n",
    "    eeg = eeg.set_index('eeg_id')\n",
    "\n",
    "    eeg = eeg.iloc[eeg_offset*200:(eeg_offset+50)*200]\n",
    "    eeg_list.append(eeg)\n",
    "\n",
    "  return dd.concat(eeg_list)\n",
    "\n",
    "ddf = load_signals(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf.compute()\n",
    "df\n",
    "eeg_ids = metadata['eeg_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig1 = df.loc['554a28223']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting top 3 channels based on max variance for all samples\n",
    "- 1000 samples computation duration = approx. 15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_extraction import calculate_all_samples\n",
    "\n",
    "top_channels_df = calculate_all_samples(df, eeg_ids, 1000) \n",
    "top_channels_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Statistical Features from every sample with extraction function\n",
    "- 1000 samples computation duration = approx. 20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_extraction import extract_features_all_samples\n",
    "\n",
    "features_df = extract_features_all_samples(df, top_channels_df)\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up Feature Data and Target Data for correct format to split data and Input to Microsoft's Light Gradient Boosting Machine (LGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index of features_df \n",
    "features_df = features_df.reset_index().rename(columns={'index': 'eeg_id'})\n",
    "\n",
    "metadata = metadata.drop_duplicates(subset='eeg_id')\n",
    "# Merging feature data with eeg data, expert consensus values converted to numerical values\n",
    "x_train = pd.merge(metadata, features_df, on='eeg_id', how='inner')\n",
    "x_train['expert_consensus'] = x_train['expert_consensus'].replace({'Seizure': 0, 'LPD': 1, 'GPD': 2,'LRDA':3, 'GRDA':4, 'Other':5 })\n",
    "\n",
    "x_train = x_train.set_index('eeg_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputting Parameters for LGBM Model\n",
    "- parameters were obtained by observing similiar implementation in same competition project using LGBM library. (see report doc --> citations/acknowledgements for more details) \n",
    "- Slight adjustments to parameters applied to fit our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 6,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'multi_logloss',\n",
    "    'num_leaves': 121,\n",
    "    'learning_rate': 0.018623105710769177,\n",
    "    'feature_fraction': 1.0,\n",
    "    'bagging_fraction': 0.756777580360579,\n",
    "    'max_depth': 8,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**params)\n",
    "# Dropping Columns that are not needed for training (Feature Data and Labels are kept)\n",
    "lgb_train = x_train.drop(columns = ['eeg_sub_id','eeg_label_offset_seconds','spectrogram_id','spectrogram_sub_id','spectrogram_label_offset_seconds','label_id', 'patient_id','seizure_vote','lpd_vote', 'gpd_vote', 'lrda_vote','grda_vote','other_vote','expert_consensus']).copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Data 80/20 and adjusting params to obtain training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# lgb_train = feature data, x_train.expert_consensus = target labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(lgb_train, x_train.expert_consensus, test_size=0.2, random_state=42)\n",
    "columns_to_convert = ['std','mean','max','min','var','med','skew','kurt','ent','mom','pow']\n",
    "X_train[columns_to_convert] = X_train[columns_to_convert].astype(float)\n",
    "\n",
    "# Setting the column names to be integers for correct indexing\n",
    "X_train.columns = range(X_train.shape[1])\n",
    "X_test.columns = range(X_test.shape[1])\n",
    "\n",
    "# Converting y_train and y_test to a one-dimensional series\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "# Setting the feature column names to be integers for correct indexing\n",
    "X_train.columns = range(X_train.shape[1])\n",
    "X_test.columns = range(X_test.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our LGBM model on the training data and evaluating it on the test data - Probabilities for each target label are obtained and Displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred_proba = lgb_model.predict_proba(X_test)\n",
    "\n",
    "# Predicted probabilities to DataFrame\n",
    "pred_df = pd.DataFrame(y_pred_proba, columns=['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote'])\n",
    "pred_df['eeg_id'] = X_test.index\n",
    "pred_df = pred_df[['eeg_id', 'seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']]\n",
    "\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Confusion Matrix with predicted labels and true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating result metrics to evaluate our multiclassification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate the metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data: Applying Wavelet Transform, Notch filter, Standardization to our data\n",
    "- Model was trained without preprocessing due to higher scores without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply the wavelet transform\n",
    "import pywt\n",
    "from src.preprocessing import wavelet_transform\n",
    "\n",
    "max_levels = pywt.dwt_max_level(data_len=10000, filter_len=pywt.Wavelet('coif1').dec_len)\n",
    "\n",
    "for sig_id in eeg_ids:\n",
    "  df.loc[sig_id] = wavelet_transform(df.loc[sig_id], max_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNE setup\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "mne_info = mne.create_info(ch_names=sig1.columns.tolist(), sfreq=200, ch_types='eeg')\n",
    "mne_info.set_montage('standard_1020')\n",
    "    \n",
    "sig1_data = np.array(sig1.transpose())\n",
    "sig1_data = np.nan_to_num(sig1_data)\n",
    "\n",
    "raw = mne.io.RawArray(sig1_data, mne_info)\n",
    "raw.apply_function(lambda x: x / 20e6, picks='eeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters\n",
    "from src.preprocessing import notch_filter, bp_filter, standardize\n",
    "\n",
    "l_freq = 1.0\n",
    "h_freq = 70.0\n",
    "\n",
    "df = notch_filter(df, 60)\n",
    "df = bp_filter(df, 1.0, 70)\n",
    "df = standardize(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File created to test the correctness of extracted values using MATLAB\n",
    "# Save Fp1 channel data into a MATLAB file\n",
    "# import scipy.io\n",
    "# scipy.io.savemat('Fp1_data.mat', {'Fp1_data': sig1['Fp1']})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.8.18"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "3.12.2"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
